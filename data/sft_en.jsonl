{"input":"In one sentence, explain gradient accumulation for training.","target":"Gradient accumulation sums losses and gradients over multiple micro-batches before a single optimizer step to emulate a larger effective batch without extra memory."}
{"input":"Define FlashAttention briefly.","target":"FlashAttention is a fused, blockwise attention kernel that reduces high-bandwidth memory traffic and intermediate activations to speed up long-context attention."}
{"input":"Why is tensor parallelism often disabled on PCIe machines?","target":"Because TP introduces per-layer synchronization that is latency-dominated on PCIe, which is hard to hide with short-to-medium sequences, leading to negative end-to-end gains."}
{"input":"What problem does FSDP solve compared to plain DDP?","target":"FSDP shards parameters, gradients, and optimizer states across ranks to reduce memory while overlapping communication with backward, enabling larger models on the same hardware."}
{"input":"What is token packing in SFT?","target":"Token packing concatenates multiple short samples into a fixed-length sequence to cut padding and raise effective tokens per step."}
{"input":"Give one advantage of bfloat16 over float32.","target":"BF16 keeps the same exponent width as FP32, preserving dynamic range, so it usually trains stably without loss scaling while reducing memory and bandwidth."}
{"input":"Describe the 1F1B pipeline schedule in a sentence.","target":"The 1F1B schedule alternates forward and backward passes of micro-batches across pipeline stages to reduce bubbles and improve hardware utilization."}
{"input":"When should sequence parallelism be enabled?","target":"Enable SP at longer contexts, typically beyond 2â€“4k tokens, when activation memory starts to dominate and its communication cost can be amortized."}
